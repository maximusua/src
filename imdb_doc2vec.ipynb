{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy\n",
    "\n",
    "# random\n",
    "from random import shuffle\n",
    "\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read complete\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import codecs\n",
    "import smart_open\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "    \n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "\n",
    "    return norm_text\n",
    "\n",
    "limit_read = 12500\n",
    "def get_big_file(dif_path):\n",
    "    temp = u''\n",
    "    read_file_count = 0\n",
    "    for file in os.listdir(dir_path) :\n",
    "        if file.endswith(\".txt\") and read_file_count < limit_read :\n",
    "            f = codecs.open(dir_path + '/' + file, \"r\")\n",
    "            read_file_count += 1;\n",
    "            t_clean = f.read()\n",
    "            for c in control_chars:\n",
    "                t_clean = t_clean.replace(c, ' ')\n",
    "            temp += t_clean\n",
    "            temp += \"\\n\"\n",
    "\n",
    "    temp_norm = normalize_text(temp)\n",
    "    return temp_norm\n",
    "\n",
    "folders_source = {'aclImdb/train/pos':'aclImdb/train-pos.txt', 'aclImdb/train/neg':'aclImdb/train-neg.txt', 'aclImdb/test/pos':'aclImdb/test-pos.txt', 'aclImdb/test/neg':'aclImdb/test-neg.txt'}\n",
    "for dir_path, file_path in folders_source.items():\n",
    "    file_result = get_big_file(dir_path)\n",
    "    with smart_open.smart_open(file_path, \"w\") as f:\n",
    "        f.write(file_result)\n",
    "print('read complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffle(self.sentences)\n",
    "        return self.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n"
     ]
    }
   ],
   "source": [
    "sources = {'aclImdb/test-neg.txt':'TEST_NEG', 'aclImdb/test-pos.txt':'TEST_POS', 'aclImdb/train-neg.txt':'TRAIN_NEG', 'aclImdb/train-pos.txt':'TRAIN_POS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)\n",
    "print('Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build vocab\n",
      "before train 0\n",
      "before train 1\n",
      "before train 2\n",
      "before train 3\n",
      "before train 4\n",
      "before train 5\n",
      "before train 6\n",
      "before train 7\n",
      "before train 8\n",
      "before train 9\n",
      "before train 10\n",
      "before train 11\n",
      "before train 12\n",
      "before train 13\n",
      "before train 14\n",
      "build doc2vec\n"
     ]
    }
   ],
   "source": [
    "# PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "# Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "# PV-DBOW \n",
    "# Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "# PV-DM w/average\n",
    "# Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "\n",
    "\n",
    "\n",
    "#model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=8)\n",
    "#PV-DBOW \n",
    "model = Doc2Vec(dm=0, min_count=1, window=10, size=100, sample=1e-4, negative=5,hs=0, workers=8)\n",
    "\n",
    "model.build_vocab(sentences.to_array())\n",
    "print('build vocab')\n",
    "for epoch in range(15):\n",
    "    print(\"before train\", epoch)\n",
    "    model.train(sentences.sentences_perm(),total_examples=model.corpus_count, epochs=epoch)\n",
    "\n",
    "#model.save('./imdb.d2v')\n",
    "model.save('./imdb_pv-dbow.d2v')\n",
    "\n",
    "\n",
    "print('build doc2vec')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = Doc2Vec.load('./imdb.d2v')\n",
    "model = Doc2Vec.load('./imdb_pv-dbow.d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(model.docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arrays = numpy.zeros((2*limit_read, 100))\n",
    "train_labels = numpy.zeros(2*limit_read)\n",
    "\n",
    "for i in range(limit_read):\n",
    "    train_arrays[i] = model.docvecs[i]\n",
    "    train_arrays[limit_read + i] = model.docvecs[limit_read + i]\n",
    "    train_labels[i] = 1\n",
    "    train_labels[limit_read + i] = 0\n",
    "\n",
    "test_arrays = numpy.zeros((2*limit_read, 100))\n",
    "test_labels = numpy.zeros(2*limit_read)\n",
    "\n",
    "for i in range(limit_read):\n",
    "    \n",
    "    test_arrays[i] = model.docvecs[2*limit_read + i]\n",
    "    test_arrays[limit_read + i] = model.docvecs[3*limit_read + i]\n",
    "    test_labels[i] = 1\n",
    "    test_labels[limit_read + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_arrays, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88624000000000003"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_arrays, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
