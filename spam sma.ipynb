{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import codecs\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "f = codecs.open(\"spam.csv\", \"r\", \"ISO-8859-1\")\n",
    "allwords = []\n",
    "\n",
    "f.readline()\n",
    "\n",
    "categ_size = 2;\n",
    "for line in f:\n",
    "    line_v = line.strip().lower().split(\",\")\n",
    "    line = ' '.join(line_v[1:])\n",
    "    words = re.split('[^a-z\\d]',line)\n",
    "    words.remove('')\n",
    "    categ_index = 0\n",
    "    if line_v[0] == 'ham':\n",
    "        categ_index = 1;\n",
    "    allwords.append([categ_index, words])\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "train_set, test_set = train_test_split(allwords, test_size = 0.2)\n",
    "\n",
    "allwords_categ = [dict(), dict()]\n",
    "documents_categ = [0, 0]\n",
    "windex = dict()\n",
    "wcount = dict()\n",
    "i = 1;\n",
    "for allword in train_set:\n",
    "    categ_index = allword[0]\n",
    "    words = allword[1]\n",
    "    documents_categ[categ_index] += 1;\n",
    "    for word in words:\n",
    "        if word not in windex :\n",
    "            windex[word] = i\n",
    "            wcount[word] = 0\n",
    "            i = i + 1\n",
    "        wcount[word] += 1\n",
    "        if word not in allwords_categ[categ_index] :\n",
    "            allwords_categ[categ_index][word] = 0\n",
    "        allwords_categ[categ_index][word] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[609, 3850]\n",
      "[[ 116.    2.]\n",
      " [  24.  973.]]\n",
      "0.983050847458\n",
      "0.828571428571\n"
     ]
    }
   ],
   "source": [
    "print(documents_categ)\n",
    "\n",
    "categ_w = np.zeros((categ_size, len(windex)+1))\n",
    "\n",
    "for word in windex:\n",
    "    for categ_index in [0, 1]:\n",
    "        if word in allwords_categ[categ_index]:\n",
    "            categ_w[categ_index][windex[word]] = math.log(allwords_categ[categ_index][word]/wcount[word])\n",
    "        else:\n",
    "            categ_w[categ_index][windex[word]] = math.log(0.0000001)\n",
    "            \n",
    "        \n",
    "eval_x = np.zeros((categ_size, categ_size))\n",
    "for t_word in test_set:\n",
    "    categ_index = t_word[0]\n",
    "    words = t_word[1]\n",
    "    test_x = np.zeros((1, len(windex)+1))\n",
    "    for word in words:\n",
    "        if word in windex:\n",
    "            test_x[0, windex[word]] = 1\n",
    "        else:\n",
    "            test_x[0, 0] = 1\n",
    "    \n",
    "    #spam - 0\n",
    "    spam_p = math.exp(np.dot(test_x, categ_w[0])[0])*(documents_categ[0]/(documents_categ[0]+documents_categ[1]))\n",
    "    ham_p = math.exp(np.dot(test_x, categ_w[1])[0])*(documents_categ[1]/(documents_categ[0]+documents_categ[1]))\n",
    "    \n",
    "    if spam_p < ham_p:\n",
    "        if categ_index == 1 :\n",
    "            eval_x[1,1] += 1\n",
    "        else:\n",
    "            eval_x[1,0] += 1\n",
    "    else:\n",
    "        if categ_index == 0 :\n",
    "            eval_x[0,0] += 1\n",
    "        else:\n",
    "            eval_x[0,1] += 1\n",
    "\n",
    "print(eval_x)\n",
    "print(eval_x[0,0]/(eval_x[0,1]+eval_x[0,0]))\n",
    "print(eval_x[0,0]/(eval_x[1,0]+eval_x[0,0]))\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
