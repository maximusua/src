{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import fasttext\n",
    "#model = fasttext.load_model('practical4/fasttext/wiki.en.bin')\n",
    "#print model.words # list of words in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss, precision_score, recall_score, f1_score\n",
    "%matplotlib inline\n",
    "\n",
    "df_orig = pd.read_csv('practical4/quora-train.csv')\n",
    "df_orig.head(5)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "df_orig.question2.replace(np.nan, 'nan', inplace=True)\n",
    "all_q = list(df_orig.question1) + list(df_orig.question2)\n",
    "tf_vectorizer = CountVectorizer()\n",
    "tf = tf_vectorizer.fit_transform(all_q)\n",
    "\n",
    "\n",
    "splitter = StratifiedShuffleSplit(\n",
    "    n_splits=1,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "ix_train, ix_test = next(splitter.split(df_orig, df_orig.is_duplicate))\n",
    "\n",
    "X = np.zeros((len(df_orig), 1))\n",
    "y = df_orig.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def norm_sentence(norm_text):\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':', '_']:\n",
    "        norm_text = norm_text.replace(char, ' ')\n",
    "    return norm_text;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabs = dict()\n",
    "f = open('practical4/quora.vocab', 'w')\n",
    "for s in all_q:\n",
    "    sentence = re.split('[^a-z\\\\d]',norm_sentence(s).lower())\n",
    "    if '' in sentence:\n",
    "        sentence.remove('')\n",
    "    for word in sentence:\n",
    "        if word not in vocabs:\n",
    "            vocabs[word] = 0\n",
    "            f.write(word+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "from gensim.models.wrappers.fasttext import FastText\n",
    "embedding_model = FastText.load_word2vec_format('practical4/quora.vec')\n",
    "\n",
    "#return embedding_model.wmdistance(doc1, doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, threshold=0.5):\n",
    "    y_pred_label = y_pred >= threshold\n",
    "    \n",
    "    print('Log loss: ', log_loss(y_true, y_pred))\n",
    "    print('Precision:', precision_score(y_true, y_pred_label))\n",
    "    print('Recall:   ', recall_score(y_true, y_pred_label))\n",
    "    print('F1 score: ', f1_score(y_true, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxgirilishen/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f26aa9f942d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtest_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_duplicate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/maxgirilishen/anaconda/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n\u001b[0;32m-> 1173\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/maxgirilishen/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/Users/maxgirilishen/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/maxgirilishen/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "q1 = list(df_orig.question1)\n",
    "q2 = list(df_orig.question2)\n",
    "    \n",
    "\n",
    "\n",
    "train_arrays = []\n",
    "train_labels = []\n",
    "test_arrays = []\n",
    "test_labels = []\n",
    "for i in range(len(q1)):\n",
    "    s_q1 = q1[i].lower();\n",
    "    s_q2 = q2[i].lower()\n",
    "    sentence_q1 = re.split('[^a-z\\\\d]',norm_sentence(s_q1))\n",
    "    sentence_q2 = re.split('[^a-z\\\\d]',norm_sentence(s_q2))\n",
    "    \n",
    "    sentence_q1 = [w for w in sentence_q1 if w not in stopwords]\n",
    "    sentence_q2 = [w for w in sentence_q2 if w not in stopwords]\n",
    "    \n",
    "    while '' in sentence_q1:\n",
    "        sentence_q1.remove('')\n",
    "    while '' in sentence_q2:\n",
    "        sentence_q2.remove('')\n",
    "   \n",
    " \n",
    "    if df_orig.id[i] in ix_train :\n",
    "        train_arrays.append(embedding_model.wmdistance(sentence_q1, sentence_q2))\n",
    "        train_labels.append(df_orig.is_duplicate[i])\n",
    "    else:\n",
    "        test_arrays.append(embedding_model.wmdistance(sentence_q1, sentence_q2))\n",
    "        test_labels.append(df_orig.is_duplicate[i])\n",
    "\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63079720003\n",
      "('Log loss: ', 12.751812952549763)\n",
      "('Precision:', 0.0)\n",
      "('Recall:   ', 0.0)\n",
      "('F1 score: ', 0.0)\n"
     ]
    }
   ],
   "source": [
    "train_arrays = np.nan_to_num(train_arrays)\n",
    "train_arrays[np.isneginf(train_arrays)] = 1\n",
    "train_arrays[np.isinf(train_arrays)] = 1\n",
    "test_arrays = np.nan_to_num(test_arrays)\n",
    "test_arrays[np.isneginf(test_arrays)] = 1\n",
    "test_arrays[np.isinf(test_arrays)] = 1\n",
    "classifier.fit(train_arrays.reshape(-1,1), train_labels)\n",
    "print(classifier.score(test_arrays.reshape(-1,1), test_labels))\n",
    "y_pred_test = classifier.predict(test_arrays.reshape(-1,1))\n",
    "evaluate(test_labels, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
